{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexMan2000/DS-GA-1003-Final-Repo/blob/main/ExplainableCNN_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explainable CNN Project Code\n"
      ],
      "metadata": {
        "id": "h79sv7nH1zlE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the code base of our DS-GA-1003 Final Project."
      ],
      "metadata": {
        "id": "vnirA2J8LrBw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Settings"
      ],
      "metadata": {
        "id": "GLm4rv5y2sBx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connect to google drive"
      ],
      "metadata": {
        "id": "ZUKTWPXW20wk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVsbrnFf5nn1"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6mlsay85931"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs('/content/gdrive/My Drive/NYU_DS_GA_1003_XAI', exist_ok=True)\n",
        "os.chdir('/content/gdrive/My Drive/NYU_DS_GA_1003_XAI')\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone this git-repo\n",
        "!git clone \"https://github.com/AlexMan2000/metrics-saliency-maps\""
      ],
      "metadata": {
        "id": "AgiHvM9OsZ37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Settings"
      ],
      "metadata": {
        "id": "N8wlNl743b6o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFilmmPP3pRT"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# download and unzip training data\n",
        "!gdown --id '1QntUQuWJoVR8h5FoeDa56xrQSdcCwFeD' --output food.zip\n",
        "!unzip food.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhzdomRTOKoJ"
      },
      "outputs": [],
      "source": [
        "# download pretrained model\n",
        "!gdown --id '1-Qw-oIJ0cSo2iG_n_U9mcJqXc2-LCSdV' --output checkpoint.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kP4zsXYaI47z"
      },
      "outputs": [],
      "source": [
        "# install lime in colab\n",
        "!pip install lime==0.1.1.37\n",
        "\n",
        "# Support for gradcam\n",
        "!pip install captum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leWC9kKgL55n"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from skimage.segmentation import slic\n",
        "from skimage.segmentation import mark_boundaries\n",
        "from lime import lime_image\n",
        "from pdb import set_trace\n",
        "from torch.autograd import Variable\n",
        "from captum.attr import LayerGradCam,LayerAttribution"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "eVkBFYZ7w5_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append(\"/content/gdrive/My Drive/NYU_DS_GA_1003_XAI/metrics-saliency-maps/saliency_maps_metrics\")\n",
        "from single_step_metrics import IIC_AD, ADD\n",
        "from multi_step_metrics import Deletion, Insertion"
      ],
      "metadata": {
        "id": "Z6HZV5djuQJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Argument Parsing"
      ],
      "metadata": {
        "id": "GVQyXl9v_c_9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbVhzRFp8dnu"
      },
      "outputs": [],
      "source": [
        "args = {\n",
        "      'ckptpath': './checkpoint.pth',\n",
        "      'dataset_dir': './food/'\n",
        "}\n",
        "args = argparse.Namespace(**args)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model and Dataset"
      ],
      "metadata": {
        "id": "BQQ00Jf0WnFr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Definition and Checkpoint Loading"
      ],
      "metadata": {
        "id": "HnoTVVRq_toZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqXQTwoxeRbO"
      },
      "outputs": [],
      "source": [
        "# Model definition\n",
        "class Classifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Classifier, self).__init__()\n",
        "\n",
        "    def building_block(indim, outdim):\n",
        "      return [\n",
        "        nn.Conv2d(indim, outdim, 3, 1, 1),\n",
        "        nn.BatchNorm2d(outdim),\n",
        "        nn.ReLU(),\n",
        "      ]\n",
        "    def stack_blocks(indim, outdim, block_num):\n",
        "      layers = building_block(indim, outdim)\n",
        "      for i in range(block_num - 1):\n",
        "        layers += building_block(outdim, outdim)\n",
        "      layers.append(nn.MaxPool2d(2, 2, 0))\n",
        "      return layers\n",
        "\n",
        "    cnn_list = []\n",
        "    cnn_list += stack_blocks(3, 128, 3)\n",
        "    cnn_list += stack_blocks(128, 128, 3)\n",
        "    cnn_list += stack_blocks(128, 256, 3)\n",
        "    cnn_list += stack_blocks(256, 512, 1)\n",
        "    cnn_list += stack_blocks(512, 512, 1)\n",
        "    self.cnn = nn.Sequential( * cnn_list)\n",
        "\n",
        "    dnn_list = [\n",
        "      nn.Linear(512 * 4 * 4, 1024),\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout(p = 0.3),\n",
        "      nn.Linear(1024, 11),\n",
        "    ]\n",
        "    self.fc = nn.Sequential( * dnn_list)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.cnn(x)\n",
        "    out = out.reshape(out.size()[0], -1)\n",
        "    return self.fc(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "er56l_dOAKBO"
      },
      "outputs": [],
      "source": [
        "# Load trained model\n",
        "model_cpu = Classifier()\n",
        "model = Classifier().cuda()\n",
        "checkpoint = torch.load(args.ckptpath)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "# It should display: <All keys matched successfully>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Definition and Creation"
      ],
      "metadata": {
        "id": "GFilFnNYAExt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iKBcxwa_Rpl"
      },
      "outputs": [],
      "source": [
        "# It might take some time, if it is too long, try to reload it.\n",
        "# Dataset definition\n",
        "class FoodDataset(Dataset):\n",
        "    def __init__(self, paths, labels, mode):\n",
        "        # mode: 'train' or 'eval'\n",
        "\n",
        "        self.paths = paths\n",
        "        self.labels = labels\n",
        "        trainTransform = transforms.Compose([\n",
        "            transforms.Resize(size=(128, 128)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomRotation(15),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "        evalTransform = transforms.Compose([\n",
        "            transforms.Resize(size=(128, 128)),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "        self.transform = trainTransform if mode == 'train' else evalTransform\n",
        "\n",
        "    # pytorch dataset class\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        X = Image.open(self.paths[index])\n",
        "        X = self.transform(X)\n",
        "        Y = self.labels[index]\n",
        "        return X, Y\n",
        "\n",
        "    # help to get images for visualizing\n",
        "    def getbatch(self, indices):\n",
        "        images = []\n",
        "        labels = []\n",
        "        for index in indices:\n",
        "          image, label = self.__getitem__(index)\n",
        "          images.append(image)\n",
        "          labels.append(label)\n",
        "        return torch.stack(images), torch.tensor(labels)\n",
        "\n",
        "# help to get data path and label\n",
        "def get_paths_labels(path):\n",
        "    def my_key(name):\n",
        "      return int(name.replace(\".jpg\",\"\").split(\"_\")[1])+1000000*int(name.split(\"_\")[0])\n",
        "    imgnames = os.listdir(path)\n",
        "    imgnames.sort(key=my_key)\n",
        "    imgpaths = []\n",
        "    labels = []\n",
        "    for name in imgnames:\n",
        "        imgpaths.append(os.path.join(path, name))\n",
        "        labels.append(int(name.split('_')[0]))\n",
        "    return imgpaths, labels\n",
        "train_paths, train_labels = get_paths_labels(args.dataset_dir)\n",
        "\n",
        "train_set = FoodDataset(train_paths, train_labels, mode='eval')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Images for Observation\n",
        "There are 11 categories of food: Bread, Dairy product, Dessert, Egg, Fried food, Meat, Noodles/Pasta, Rice, Seafood, Soup, and Vegetable/Fruit.\n",
        "The images are marked from 0 to 9."
      ],
      "metadata": {
        "id": "DHj9yvMsAumF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKFmM1sacjyG"
      },
      "outputs": [],
      "source": [
        "img_indices = [i for i in range(10)]\n",
        "images, labels = train_set.getbatch(img_indices)\n",
        "fig, axs = plt.subplots(1, len(img_indices), figsize=(15, 8))\n",
        "for i, img in enumerate(images):\n",
        "  axs[i].imshow(img.cpu().permute(1, 2, 0))\n",
        "# print(labels) # this line can help you know the labels of each image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images.shape"
      ],
      "metadata": {
        "id": "8mQpiubN802F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preliminaries: What are we doing?\n",
        "Basically we want to evaluate the faithfulness(e.g. reliability) of the CNN explanation methods(e.g. Saliency Map, GradCam).\n",
        "\n",
        "\n",
        "Faithfulness means to what extent I can trust saliency map to explain my CNN model. Does its explanation aligns with my judgement.\n",
        "\n",
        "The image below shows the gist of our project.\n",
        "\n"
      ],
      "metadata": {
        "id": "tfaxP_cYBlMZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://github.com/TristanGomez44/metrics-saliency-maps/raw/main/pics/metrics_repo_illust.png)"
      ],
      "metadata": {
        "id": "9tm0IEqTBrw_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What are those XAI Algorithms?\n",
        "\n",
        "Those XAI Algorithms are responsible for explaining what a CNN model has learned in the image classification task.\n",
        "\n",
        "\n",
        "\n",
        "For example in the image above.\n",
        "- As a human being, I will say since I see the bird in the image, so I classify the image as bird. From my perspective, something in the image that looks like a \"bird\" makes me think and classify it as a bird.\n",
        "- As a machine learning model, it may think that  I learn some features(e.g. model parameters like kernel filters $W$ or bias $b$) from the image that makes me classify it as a bird. For example, since the model sees certain pixels in the image(e.g. pixels around bird's beak), it classifies this image as a bird.\n",
        "\n",
        "\n",
        "So a model must have \"see\" some portion of the image so that it can correctly classify an input image.\n",
        "\n",
        "Those algorithms are responsible for finding what on earth the model is \"see\"ing.\n",
        "\n",
        "\n",
        "Algorithm ideas are detailed in XAI Part 1.\n"
      ],
      "metadata": {
        "id": "m4q0ck2yE97x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to evaluate?\n",
        "\n",
        "For those XAI Algorithms, we will have a set of evaluation metrics(e.g. DAUC, IAUC, DC, IC) that aims to quantify their performance."
      ],
      "metadata": {
        "id": "XYZzu2SBFCC_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Terminology\n",
        "\n",
        "- Explanation Method:\n",
        "- Heatmap:\n",
        "- Saliency Map\n",
        "\n"
      ],
      "metadata": {
        "id": "UsII3_v-LjGY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **XAI Part 1: CNN Heatmap XAI Algorithms**"
      ],
      "metadata": {
        "id": "EkQZ-kVUAVEb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lime Package\n",
        "[Lime](https://github.com/marcotcr/lime) is a package about explaining what machine learning classifiers are doing. We can first use it to observe the model."
      ],
      "metadata": {
        "id": "nGGVJx9IBe8F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RI6e9_68HvQe"
      },
      "outputs": [],
      "source": [
        "def predict(input):\n",
        "    # input: numpy array, (batches, height, width, channels)\n",
        "\n",
        "    model.eval()\n",
        "    input = torch.FloatTensor(input).permute(0, 3, 1, 2)\n",
        "    # pytorch tensor, (batches, channels, height, width)\n",
        "\n",
        "    output = model(input.cuda())\n",
        "    return output.detach().cpu().numpy()\n",
        "\n",
        "def segmentation(input):\n",
        "    # split the image into 200 pieces with the help of segmentaion from skimage\n",
        "    return slic(input, n_segments=200, compactness=1, sigma=1, start_label=1)\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(1, len(img_indices), figsize=(15, 8))\n",
        "# fix the random seed to make it reproducible\n",
        "np.random.seed(16)\n",
        "for idx, (image, label) in enumerate(zip(images.permute(0, 2, 3, 1).numpy(), labels)):\n",
        "    x = image.astype(np.double)\n",
        "    # numpy array for lime\n",
        "\n",
        "    explainer = lime_image.LimeImageExplainer()\n",
        "    explaination = explainer.explain_instance(image=x, classifier_fn=predict, segmentation_fn=segmentation)\n",
        "\n",
        "    # doc: https://lime-ml.readthedocs.io/en/latest/lime.html?highlight=explain_instance#lime.lime_image.LimeImageExplainer.explain_instance\n",
        "\n",
        "    lime_img, mask = explaination.get_image_and_mask(\n",
        "                                label=label.item(),\n",
        "                                positive_only=False,\n",
        "                                hide_rest=False,\n",
        "                                num_features=11,\n",
        "                                min_weight=0.05\n",
        "                            )\n",
        "    # turn the result from explainer to the image\n",
        "    # doc: https://lime-ml.readthedocs.io/en/latest/lime.html?highlight=get_image_and_mask#lime.lime_image.ImageExplanation.get_image_and_mask\n",
        "\n",
        "    axs[idx].imshow(lime_img)\n",
        "\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saliency Map(Baseline)"
      ],
      "metadata": {
        "id": "zctMN1bdCTVY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is Saliency map ?\n",
        "\n",
        "The heatmaps that highlight pixels of the input image that contribute the most in the classification task.\n",
        "\n",
        "Ref: https://medium.com/datadriveninvestor/visualizing-neural-networks-using-saliency-maps-in-pytorch-289d8e244ab4"
      ],
      "metadata": {
        "id": "EYDU5l9PCc6B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We put an image into the model, forward then calculate the loss referring to the label. Therefore, the loss is related to:\n",
        "\n",
        "\n",
        "*   image\n",
        "*   model parameters\n",
        "*   label\n",
        "\n",
        "Generally speaking, we change model parameters to fit \"image\" and \"label\". When backward, we calculate the partial differential value of **loss to model parameters**.\n",
        "\n",
        "Now, we have another look. When we change the image's pixel value, the partial differential value of **loss to image** shows the change in the loss. We can say that it means the importance of the pixel. We can visualize it to demonstrate which part of the image contribute the most to the model's judgment."
      ],
      "metadata": {
        "id": "SazwNhS4ChoO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swIZSW7O04O-"
      },
      "outputs": [],
      "source": [
        "def normalize(image):\n",
        "  return (image - image.min()) / (image.max() - image.min())\n",
        "  # return torch.log(image)/torch.log(image.max())\n",
        "\n",
        "def compute_saliency_maps(x, y, model):\n",
        "  model.eval()\n",
        "  x = x.cuda()\n",
        "\n",
        "  # we want the gradient of the input x\n",
        "  x.requires_grad_()\n",
        "\n",
        "\n",
        "  y_pred = model(x)\n",
        "\n",
        "\n",
        "  loss_func = torch.nn.CrossEntropyLoss()\n",
        "  loss = loss_func(y_pred, y.cuda())\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  # saliencies = x.grad.abs().detach().cpu()\n",
        "\n",
        "  # Saliency would be the gradient with respect to the input image now. But note that the input image has 3 channels,\n",
        "  # R, G and B. To derive a single class saliency value for each pixel (i, j),  we take the maximum magnitude\n",
        "  # across all colour channels.\n",
        "  saliencies, _ = torch.max(x.grad.data.abs().detach().cpu(),dim=1)\n",
        "\n",
        "  # We need to normalize each image, because their gradients might vary in scale\n",
        "  saliencies = torch.stack([normalize(item) for item in saliencies])\n",
        "  return saliencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_w8iSe319Ws"
      },
      "outputs": [],
      "source": [
        "# images, labels = train_set.getbatch(img_indices)\n",
        "saliencies = compute_saliency_maps(images, labels, model)\n",
        "\n",
        "# visualize\n",
        "fig, axs = plt.subplots(2, len(img_indices), figsize=(15, 8))\n",
        "for row, target in enumerate([images, saliencies]):\n",
        "  for column, img in enumerate(target):\n",
        "    if row==0:\n",
        "      axs[row][column].imshow(img.permute(1, 2, 0).numpy())\n",
        "      # What is permute?\n",
        "      # In pytorch, the meaning of each dimension of image tensor is (channels, height, width)\n",
        "      # In matplotlib, the meaning of each dimension of image tensor is (height, width, channels)\n",
        "      # permute is a tool for permuting dimensions of tensors\n",
        "      # For example, img.permute(1, 2, 0) means that,\n",
        "      # - 0 dimension is the 1 dimension of the original tensor, which is height\n",
        "      # - 1 dimension is the 2 dimension of the original tensor, which is width\n",
        "      # - 2 dimension is the 0 dimension of the original tensor, which is channels\n",
        "    else:\n",
        "      axs[row][column].imshow(img.numpy(), cmap=plt.cm.hot)\n",
        "\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Smooth Grad\n",
        "\n",
        "The method of Smooth grad is to randomly add noise to the image and get different heatmaps. The average of the heatmaps would be more robust to noisy gradient.\n",
        "\n",
        "ref: https://arxiv.org/pdf/1706.03825.pdf"
      ],
      "metadata": {
        "id": "IAslTKyuDIvq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SmoothGrad works in the following way:\n",
        "1. Generate multiple versions of the image of interest by adding noise to it.\n",
        "2. Create pixel attribution maps for all images.\n",
        "3. Average the pixel attribution maps.\n",
        "\n",
        "Yes, it is that simple. Why should this work? The theory is that the derivative fluctuates greatly at small scales. Neural networks have no incentive during training to keep the gradients smooth, their goal is to classify images correctly. Averaging over multiple maps \"smooths out\" these fluctuations:\n",
        "$$\n",
        "R_{s g}(x)=\\frac{1}{N} \\sum_{i=1}^n R\\left(x+g_i\\right),\n",
        "$$\n",
        "\n",
        "Here, $g_i \\sim N\\left(0, \\sigma^2\\right)$ are noise vectors sampled from the Gaussian distribution. The \"ideal\" noise level depends on the input image and the network. The authors suggest a noise level of $10 \\%-20 \\%$, which means that $\\frac{\\sigma}{x_{\\max }-x_{\\min }}$ should be between 0.1 and 0.2 . The limits $x_{\\min }$ and $x_{\\max }$ refer to minimum and maximum pixel values of the image. The other parameter is the number of samples $n$, for which was suggested to use $n=50$, since there are diminishing returns above that."
      ],
      "metadata": {
        "id": "MqrehYKZAGye"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjusfKjISm-k"
      },
      "outputs": [],
      "source": [
        "# Smooth grad\n",
        "\n",
        "def normalize(image):\n",
        "  return (image - image.min()) / (image.max() - image.min())\n",
        "\n",
        "def smooth_grad(x, y, model, epoch, param_sigma_multiplier):\n",
        "  model.eval()\n",
        "  # x = x.cuda().unsqueeze(0)\n",
        "\n",
        "  mean = 0\n",
        "  sigma = param_sigma_multiplier / (torch.max(x) - torch.min(x)).item()\n",
        "  smooth = np.zeros(x.cuda().unsqueeze(0).size())\n",
        "  for i in range(epoch):\n",
        "    # call Variable to generate random noise\n",
        "    noise = Variable(x.data.new(x.size()).normal_(mean, sigma**2))\n",
        "    x_mod = (x+noise).unsqueeze(0).cuda()\n",
        "    x_mod.requires_grad_()\n",
        "\n",
        "    y_pred = model(x_mod)\n",
        "    loss_func = torch.nn.CrossEntropyLoss()\n",
        "    loss = loss_func(y_pred, y.cuda().unsqueeze(0))\n",
        "    loss.backward()\n",
        "\n",
        "    # like the method in saliency map\n",
        "    smooth += x_mod.grad.abs().detach().cpu().data.numpy()\n",
        "\n",
        "  smooth = normalize(smooth / epoch) # don't forget to normalize\n",
        "\n",
        "\n",
        "  # Maximum value across all channels to obtain a singla value.\n",
        "  torch_smooth = torch.max(torch.tensor(smooth, dtype=torch.float32), dim=1)\n",
        "  smooth = torch_smooth[0].numpy()\n",
        "  # smooth = smooth / epoch\n",
        "  return smooth\n",
        "\n",
        "def trigger():\n",
        "  # images, labels = train_set.getbatch(img_indices)\n",
        "  smooth = []\n",
        "  for i, l in zip(images, labels):\n",
        "    smooth.append(smooth_grad(i, l, model, 500, 0.4))\n",
        "  smooth = np.stack(smooth)\n",
        "  # print(smooth.shape)\n",
        "\n",
        "  fig, axs = plt.subplots(2, len(img_indices), figsize=(15, 8))\n",
        "  for row, target in enumerate([images, smooth]):\n",
        "    for column, img in enumerate(target):\n",
        "      if row==0:\n",
        "\n",
        "        axs[row][column].imshow(img.permute(1, 2, 0).numpy())\n",
        "      else:\n",
        "        axs[row][column].imshow(img.transpose((1,2,0)), cmap=plt.cm.hot)\n",
        "\n",
        "  return smooth\n",
        "\n",
        "smooth = trigger()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Z9o2c3jTlwG"
      },
      "source": [
        "## Filter Explanation\n",
        "\n",
        "In this part, we want to know what a specific filter recognize, we'll do\n",
        "- Filter activation: pick up some images, and check which part of the image activates the filter\n",
        "- Filter visualization: look for which kind of image can activate the filter the most\n",
        "\n",
        "\n",
        "The problem is that, in normal case, we'll directly feed the image to the model, for example,\n",
        "```\n",
        "loss = model(image)\n",
        "loss.backward()\n",
        "```\n",
        "\n",
        "How can we get the output of a specific layer of CNN?\n",
        "\n",
        "We can modify the model definition, make the forward function not only return loss but also retrun the activation map. But this is difficult to maintain the code. As a result, pytorch offers a better solution: **hook**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVJlyj01b_A4"
      },
      "outputs": [],
      "source": [
        "def normalize(image):\n",
        "  return (image - image.min()) / (image.max() - image.min())\n",
        "\n",
        "layer_activations = None\n",
        "def filter_explanation(x, model, cnnid, filterid, iteration=100, lr=1):\n",
        "  # x: input image\n",
        "  # cnnid: cnn layer id\n",
        "  # filterid: which filter\n",
        "  model.eval()\n",
        "\n",
        "  def hook(model, input, output):\n",
        "    global layer_activations\n",
        "    layer_activations = output\n",
        "\n",
        "  hook_handle = model.cnn[cnnid].register_forward_hook(hook)\n",
        "  # When the model forwards through the layer[cnnid], it needs to call the hook function first\n",
        "  # The hook function save the output of the layer[cnnid]\n",
        "  # After forwarding, we'll have the loss and the layer activation\n",
        "\n",
        "  # Filter activation: x passing the filter will generate the activation map\n",
        "  model(x.cuda()) # forward\n",
        "\n",
        "  # Based on the filterid given by the function argument, pick up the specific filter's activation map\n",
        "  # We just need to plot it, so we can detach from graph and save as cpu tensor\n",
        "  filter_activations = layer_activations[:, filterid, :, :].detach().cpu()\n",
        "\n",
        "  # Filter visualization: find the image that can activate the filter the most\n",
        "  x = x.cuda()\n",
        "  x.requires_grad_()\n",
        "  # input image gradient\n",
        "  optimizer = Adam([x], lr=lr)\n",
        "  # Use optimizer to modify the input image to amplify filter activation\n",
        "  for iter in range(iteration):\n",
        "    optimizer.zero_grad()\n",
        "    model(x)\n",
        "\n",
        "    objective = -layer_activations[:, filterid, :, :].sum()\n",
        "    # We want to maximize the filter activation's summation\n",
        "    # So we add a negative sign\n",
        "\n",
        "    objective.backward()\n",
        "    # Calculate the partial differential value of filter activation to input image\n",
        "    optimizer.step()\n",
        "    # Modify input image to maximize filter activation\n",
        "  filter_visualizations = x.detach().cpu().squeeze()\n",
        "\n",
        "  # Don't forget to remove the hook\n",
        "  hook_handle.remove()\n",
        "  # The hook will exist after the model register it, so you have to remove it after used\n",
        "  # Just register a new hook if you want to use it\n",
        "\n",
        "  return filter_activations, filter_visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7Q-0mxV-xoo"
      },
      "outputs": [],
      "source": [
        "images, labels = train_set.getbatch(img_indices)\n",
        "filter_activations, filter_visualizations = filter_explanation(images, model, cnnid=6, filterid=0, iteration=100, lr=0.1)\n",
        "\n",
        "fig, axs = plt.subplots(3, len(img_indices), figsize=(15, 8))\n",
        "for i, img in enumerate(images):\n",
        "  axs[0][i].imshow(img.permute(1, 2, 0))\n",
        "# Plot filter activations\n",
        "for i, img in enumerate(filter_activations):\n",
        "  axs[1][i].imshow(normalize(img))\n",
        "# Plot filter visualization\n",
        "for i, img in enumerate(filter_visualizations):\n",
        "  axs[2][i].imshow(normalize(img.permute(1, 2, 0)))\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAaUtuvl7Chg"
      },
      "outputs": [],
      "source": [
        "images, labels = train_set.getbatch(img_indices)\n",
        "filter_activations, filter_visualizations = filter_explanation(images, model, cnnid=23, filterid=0, iteration=100, lr=0.1)\n",
        "\n",
        "# Plot filter activations\n",
        "fig, axs = plt.subplots(3, len(img_indices), figsize=(15, 8))\n",
        "for i, img in enumerate(images):\n",
        "  axs[0][i].imshow(img.permute(1, 2, 0))\n",
        "for i, img in enumerate(filter_activations):\n",
        "  axs[1][i].imshow(normalize(img))\n",
        "for i, img in enumerate(filter_visualizations):\n",
        "  axs[2][i].imshow(normalize(img.permute(1, 2, 0)))\n",
        "plt.show()\n",
        "plt.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Integrated Gradients\n",
        "https://arxiv.org/pdf/1703.01365.pdf\n"
      ],
      "metadata": {
        "id": "VgytLPR5Gw1c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SK3JVklGiqw6"
      },
      "outputs": [],
      "source": [
        "class IntegratedGradients():\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.gradients = None\n",
        "        # Put model in evaluation mode\n",
        "        self.model.eval()\n",
        "\n",
        "    def generate_images_on_linear_path(self, input_image, steps):\n",
        "        # Generate scaled xbar images\n",
        "        xbar_list = [input_image*step/steps for step in range(steps)]\n",
        "        return xbar_list\n",
        "\n",
        "    def generate_gradients(self, input_image, target_class):\n",
        "        # We want to get the gradients of the input image\n",
        "        input_image.requires_grad=True\n",
        "        # Forward\n",
        "        model_output = self.model(input_image)\n",
        "        # Zero grads\n",
        "        self.model.zero_grad()\n",
        "        # Target for backprop\n",
        "        one_hot_output = torch.FloatTensor(1, model_output.size()[-1]).zero_().cuda()\n",
        "        one_hot_output[0][target_class] = 1\n",
        "        # Backward\n",
        "        model_output.backward(gradient=one_hot_output)\n",
        "        self.gradients = input_image.grad\n",
        "        # Convert Pytorch variable to numpy array\n",
        "        # [0] to get rid of the first channel (1,3,128,128)\n",
        "        gradients_as_arr = self.gradients.data.cpu().numpy()[0]\n",
        "        return gradients_as_arr\n",
        "\n",
        "    def generate_integrated_gradients(self, input_image, target_class, steps):\n",
        "        # Generate xbar images\n",
        "        xbar_list = self.generate_images_on_linear_path(input_image, steps)\n",
        "        # Initialize an image composed of zeros\n",
        "        integrated_grads = np.zeros(input_image.size())\n",
        "        for xbar_image in xbar_list:\n",
        "            # Generate gradients from xbar images\n",
        "            single_integrated_grad = self.generate_gradients(xbar_image, target_class)\n",
        "            # Add rescaled grads from xbar images\n",
        "            integrated_grads = integrated_grads + single_integrated_grad/steps\n",
        "        # [0] to get rid of the first channel (1,3,128,128)\n",
        "\n",
        "\n",
        "        return integrated_grads[0]\n",
        "\n",
        "def normalize(image):\n",
        "  return (image - image.min()) / (image.max() - image.min())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiVRoNgFwTY1"
      },
      "outputs": [],
      "source": [
        "# put the image to cuda\n",
        "images, labels = train_set.getbatch(img_indices)\n",
        "images = images.cuda()\n",
        "\n",
        "IG = IntegratedGradients(model)\n",
        "integrated_grads = []\n",
        "for i, img in enumerate(images):\n",
        "  img = img.unsqueeze(0)\n",
        "  integrated_grads.append(IG.generate_integrated_gradients(img, labels[i], 10))\n",
        "\n",
        "fig, axs = plt.subplots(2, len(img_indices), figsize=(15, 8))\n",
        "for i, img in enumerate(images):\n",
        "  axs[0][i].imshow(img.cpu().permute(1, 2, 0))\n",
        "for i, img in enumerate(integrated_grads):\n",
        "  axs[1][i].imshow(np.moveaxis(normalize(img),0,-1))\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grad-CAM\n",
        "https://christophm.github.io/interpretable-ml-book/pixel-attribution.html#vanilla-gradient-saliency-maps"
      ],
      "metadata": {
        "id": "hYbAGQY-zROA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following is the recipe for Grad-CAM. Our goal is to find the localization map, which is defined as:\n",
        "$$\n",
        "L_{\\text {Grad-CAM }}^c \\in \\mathbb{R}^{u \\times v}=\\underbrace{\\operatorname{ReLU}}_{\\text {Pick positive values }}\\left(\\sum_k \\alpha_k^c A^k\\right)\n",
        "$$\n",
        "\n",
        "Here, $u$ is the width, $v$ the height of the explanation and $c$ the class of interest.\n",
        "1. Forward-propagate the input image through the convolutional neural network.\n",
        "2. Obtain the raw score for the class of interest, meaning the activation of the neuron before the softmax layer.\n",
        "3. Set all other class activations to zero.\n",
        "4. Back-propagate the gradient of the class of interest to the last convolutional layer before the fully connected layers: $\\frac{\\delta y^c}{\\delta A^k}$.\n",
        "5. Weight each feature map \"pixel\" by the gradient for the class. Indices $i$ and $j$ refer to the width and height dimensions:\n",
        "$$\n",
        "\\alpha_k^c=\\overbrace{\\frac{1}{Z} \\sum_i \\sum_j}^{\\text {global average pooling }} \\underbrace{\\frac{\\delta y^c}{\\delta A_{i j}^k}}_{\\text {gradients via backprop }}\n",
        "$$\n",
        "\n",
        "This means that the gradients are globally pooled.\n",
        "6. Calculate an average of the feature maps, weighted per pixel by the gradient.\n",
        "7. Apply ReLU to the averaged feature map.\n",
        "8. For visualization: Scale values to the interval between 0 and 1 . Upscale the image and overlay it over the original image.\n",
        "9. Additional step for Guided Grad-CAM: Multiply heatmap with guided backpropagation."
      ],
      "metadata": {
        "id": "JI67y74p7t0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images, labels = train_set.getbatch(img_indices)\n",
        "\n",
        "def compute_gradcam(x, y, cnnid, model):\n",
        "  model = model.eval()\n",
        "  x = x.cuda()\n",
        "\n",
        "  layer_to_analyze = model.cnn[cnnid]\n",
        "  gradcam = LayerGradCam(model, layer_to_analyze)\n",
        "\n",
        "  # Step 1,2 Obtain relu output before softmax\n",
        "  class_ind = model(x).argmax(dim=-1)\n",
        "\n",
        "  # Step 3,4: Calculate the gradient to the layer's feature map specified by cnn[cnnid]\n",
        "  attr = gradcam.attribute(x, class_ind)\n",
        "\n",
        "  # Step 5~7: Upsample to the size of input image\n",
        "  upsampled_attr = LayerAttribution.interpolate(attr, (128, 128))\n",
        "\n",
        "  # Step 8 :Scale the value to between [0, 1]\n",
        "  upsampled_attr = (upsampled_attr - upsampled_attr.min())/(upsampled_attr.max()-upsampled_attr.min())\n",
        "\n",
        "  # Normalize the image\n",
        "  x = (x - x.min())/(x.max()-x.min())\n",
        "\n",
        "  # Apply the mask\n",
        "  img_to_viz = (upsampled_attr * x.mean(dim=1,keepdim=True)).detach().cpu()\n",
        "\n",
        "  return upsampled_attr, img_to_viz\n",
        "\n",
        "\n",
        "def trigger():\n",
        "  attr, img_to_viz = compute_gradcam(images, labels, 6, model)\n",
        "\n",
        "  # Plot filter activations\n",
        "  fig, axs = plt.subplots(2, len(img_indices), figsize=(15, 8))\n",
        "  for i, img in enumerate(images):\n",
        "    axs[0][i].imshow(img.permute(1, 2, 0))\n",
        "  for i, img in enumerate(img_to_viz):\n",
        "    axs[1][i].imshow(img.permute(1, 2, 0))\n",
        "\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "  return attr, img_to_viz\n",
        "\n",
        "gradcam_expl, img_to_viz = trigger()"
      ],
      "metadata": {
        "id": "Re-QqrZuzQpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model(images[0].unsqueeze(0).cuda())"
      ],
      "metadata": {
        "id": "KpT2yH-P2TDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Organize the plot"
      ],
      "metadata": {
        "id": "UcGOOI6_WSfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# for i, img in enumerate(img_to_viz):\n",
        "#   axs[5, i].imshow(img.permute(1, 2, 0))"
      ],
      "metadata": {
        "id": "X3zPDFupWUn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XAI Part 2 Evaluations\n"
      ],
      "metadata": {
        "id": "Fz58_NQ3RPsm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Methods"
      ],
      "metadata": {
        "id": "cJaghZU9WIR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper functions\n",
        "def multi_step_ingredients(images, labels, model, algo_mode):\n",
        "\n",
        "  assert algo_mode in [\"Grad_CAM\", \"Saliency_Map\", \"Smooth_Grad\", \"Integrated_Grad\"]\n",
        "\n",
        "  allImg = []\n",
        "  allExpl = []\n",
        "  allInds = []\n",
        "\n",
        "  for ind, img in enumerate(images):\n",
        "\n",
        "      img = img.unsqueeze(0)\n",
        "      class_ind = model(img.cuda()).argmax(dim=-1)\n",
        "\n",
        "      # print(\"input image\", img.shape)\n",
        "\n",
        "\n",
        "      # Get the exaplanation map, should be of shape (1, 1, 128, 128) <-> (N, C, H, W)\n",
        "      if algo_mode == \"Grad_CAM\":\n",
        "        explanation,_ = compute_gradcam(img, class_ind, 6, model)\n",
        "      elif algo_mode == \"Saliency_Map\":\n",
        "        explanation = compute_saliency_maps(img, labels[ind].unsqueeze(0), model)\n",
        "        explanation = explanation.unsqueeze(0)\n",
        "      elif algo_mode == \"Smooth_Grad\":\n",
        "        explanation = smooth_grad(img.squeeze(0), labels[ind], model, 500, 0.4)\n",
        "        explanation = torch.tensor(explanation).unsqueeze(0)\n",
        "        # print(\"expl\", explanation.shape)\n",
        "      elif algo_mode == \"Integrated_Grad\":\n",
        "        IG = IntegratedGradients(model)\n",
        "        # Output is (3, 128, 128) on numpy\n",
        "        explanation = IG.generate_integrated_gradients(img.cuda(), labels[ind], 10)\n",
        "        # Transform into a single gradient map as before (1, 1, 128, 128)\n",
        "        explanation = torch.max(torch.tensor(explanation, dtype=torch.float32).unsqueeze(0), dim=1)[0].unsqueeze(0)\n",
        "\n",
        "      allImg.append(img.cuda())\n",
        "      allExpl.append(explanation)\n",
        "      allInds.append(class_ind)\n",
        "\n",
        "  allImg = torch.cat(allImg,dim=0)\n",
        "  allExpl = torch.cat(allExpl,dim=0)\n",
        "  allInds = torch.cat(allInds,dim=0)\n",
        "\n",
        "  return allImg, allExpl, allInds\n",
        "\n",
        "\n",
        "# Given a batch of images and their labels, evaluate the\n",
        "# explanation methods(algo_mode) with a particular evaluation metric(eval_mode)\n",
        "def computeMetrics(images, labels, model, algo_mode):\n",
        "  allImg, allExpl, allInds = multi_step_ingredients(images, labels, model, algo_mode)\n",
        "\n",
        "  deletion = Deletion()\n",
        "  result_dic, dauc_list, dc_list = deletion(model,allImg.clone(),allExpl.clone(),allInds)\n",
        "  dauc_mean = result_dic[\"dauc\"]\n",
        "  dc_mean = result_dic[\"dc\"]\n",
        "\n",
        "  insertion = Insertion()\n",
        "  result_dic, iauc_list, ic_list = insertion(model,allImg.clone(),allExpl.clone(),allInds)\n",
        "  iauc_mean = result_dic[\"iauc\"]\n",
        "  ic_mean = result_dic[\"ic\"]\n",
        "\n",
        "\n",
        "  iic_ad = IIC_AD()\n",
        "  result_dic, iic_list, ad_list = iic_ad(model,allImg,allExpl,allInds)\n",
        "  iic_mean,ad_mean = result_dic[\"iic\"],result_dic[\"ad\"]\n",
        "\n",
        "  add = ADD()\n",
        "  result_dic, add_list = add(model,allImg,allExpl,allInds)\n",
        "  add_mean = result_dic[\"add\"]\n",
        "\n",
        "\n",
        "  result_dict = {\n",
        "            \"DAUC\": dauc_mean\n",
        "            ,\"IAUC\": iauc_mean\n",
        "            ,\"DC\": dc_mean\n",
        "            ,\"IC\":ic_mean\n",
        "            ,\"IIC\": iic_mean\n",
        "            ,\"AD\": ad_mean\n",
        "            ,\"ADD\": add_mean\n",
        "          }\n",
        "\n",
        "\n",
        "  result_dict_detailed = {\n",
        "            \"DAUC\": dauc_list\n",
        "            ,\"IAUC\": iauc_list\n",
        "            ,\"DC\": dc_list\n",
        "            ,\"IC\": ic_list\n",
        "            ,\"IIC\": iic_list\n",
        "            ,\"AD\": ad_list\n",
        "            ,\"ADD\": add_list\n",
        "          }\n",
        "\n",
        "\n",
        "  return result_dict, result_dict_detailed"
      ],
      "metadata": {
        "id": "drUzz3zALFpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def temp():\n",
        "  res = {\"auc\":[1,2,3], \"ic\":[2,23,4]}\n",
        "  return 1, *list(res.values())\n",
        "temp()"
      ],
      "metadata": {
        "id": "ZuhdZtR21wE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compute Evaluation"
      ],
      "metadata": {
        "id": "elB8VIBnzeGX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trigger Codes"
      ],
      "metadata": {
        "id": "zuagtD-s3Xz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GradCAM_dict_simple, GradCAM_dict_detailed = computeMetrics(images, labels, model, \"Grad_CAM\")"
      ],
      "metadata": {
        "id": "fWbW7rwyX8GS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Saliency_Map_dict_simple, Saliency_Map_dict_detailed = computeMetrics(images, labels, model, \"Saliency_Map\")"
      ],
      "metadata": {
        "id": "hjATz5lqcF6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Smooth_Grad_dict_simple, Smooth_Grad_dict_detailed = computeMetrics(images, labels, model, \"Smooth_Grad\")"
      ],
      "metadata": {
        "id": "cgF2SMmShfkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Integrated_Grad_dict_simple, Integrated_Grad_dict_detailed = computeMetrics(images, labels, model, \"Integrated_Grad\")"
      ],
      "metadata": {
        "id": "kLoVfdtyx2f5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GradCAM_dict_detailed"
      ],
      "metadata": {
        "id": "9GvsTgM93x3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Integrated_Grad_dict_detailed"
      ],
      "metadata": {
        "id": "XdiMX_r66Bob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Area-Based Metrics"
      ],
      "metadata": {
        "id": "XiULFDKE3b3r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DAUC\n",
        "\n",
        "The intuition behind this is that if a saliency map highlights\n",
        "the areas that are relevant to the decision, masking them will result in a large\n",
        "decrease of the initially predicted class score, which in turn will minimize the\n",
        "AUC. **Therefore, minimizing this metric corresponds to an improvement.**"
      ],
      "metadata": {
        "id": "yrOxkW42rUa9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IAUC\n",
        "\n",
        "Instead of progressively masking the image, the IAUC metric starts from a\n",
        "blurred image and then progressively unblurs it by starting from the most important areas according to the saliency map. Similarly, if the areas highlighted\n",
        "by the map are relevant for predicting the correct category, the score of the corresponding class (obtained using the partially unblurred image) is supposed to\n",
        "increase rapidly. **Conversely, maximizing this metric corresponds to an improvement.**"
      ],
      "metadata": {
        "id": "P4HwuLFjrV80"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Limitations\n",
        "\n",
        "**DAUC and IAUC generate out of distribution (OOD) images**\n",
        "\n",
        "When\n",
        "progressively masking/unblurring the input image, the model is presented with\n",
        "samples that can be considered out of the training distribution.\n",
        "\n",
        "This kind of distortions produced by the masking/blurring operations\n",
        "do not exist naturally in the dataset and are different from the kind produced\n",
        "by the standard data augmentations like random crop, horizontal flip, and color\n",
        "jitter, meaning that the model has not learned to process images with such\n",
        "distortions. Therefore, the distribution of the images presented to the model is\n",
        "different from the one met during training.\n",
        "\n",
        "This shows that DAUC and IAUC may not reflect\n",
        "the faithfulness of explanation methods as they are based on a behavior of the\n",
        "model that is different from that encountered when facing training distribution"
      ],
      "metadata": {
        "id": "b4nFysQII_0q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calibration-Based Metric"
      ],
      "metadata": {
        "id": "Xlt2jWCAzwT2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deletion Correlation(DC)"
      ],
      "metadata": {
        "id": "6Hxs2v2graqz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Insertion Correlation(IC)"
      ],
      "metadata": {
        "id": "86V6eflQrdRT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Organize the Output"
      ],
      "metadata": {
        "id": "AzZm8ZC720by"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(5, len(img_indices), figsize=(20, 12))\n",
        "fig.tight_layout(pad=1.2)\n",
        "for i, img in enumerate(images):\n",
        "  if i == 5:\n",
        "    axs[0, i].set_title(\"Original Image\")\n",
        "  axs[0, i].imshow(img.permute(1, 2, 0))\n",
        "  axs[0, i].xaxis.set_visible(False)\n",
        "  axs[0, i].yaxis.set_visible(False)\n",
        "\n",
        "for i, img in enumerate(saliencies):\n",
        "  if i == 5:\n",
        "    axs[1, i].set_title(\"Saliency Map\")\n",
        "  axs[1, i].imshow(img.numpy(), cmap=plt.cm.hot)\n",
        "  # axs[1, i].xaxis.set_visible(False)\n",
        "  axs[1, i].yaxis.set_visible(False)\n",
        "  axs[1, i].set_xlabel(f\"DAUC:{round(Saliency_Map_dict_detailed['DAUC'][i], 2)}\\n IAUC:{round(Saliency_Map_dict_detailed['IAUC'][i], 2)} \\n DC:{round(Saliency_Map_dict_detailed['DC'][i], 2)}\")\n",
        "\n",
        "\n",
        "\n",
        "for i, img in enumerate(smooth):\n",
        "  if i == 5:\n",
        "    axs[2, i].set_title(\"Smooth Grad\")\n",
        "  axs[2, i].imshow(img.transpose((1,2,0)),cmap=plt.cm.hot)\n",
        "  # axs[2, i].xaxis.set_visible(False)\n",
        "  axs[2, i].yaxis.set_visible(False)\n",
        "  axs[2, i].set_xlabel(f\"DAUC:{round(Smooth_Grad_dict_detailed['DAUC'][i], 2)}\\n IAUC:{round(Smooth_Grad_dict_detailed['IAUC'][i], 2)} \\n DC:{round(Smooth_Grad_dict_detailed['DC'][i], 2)}\")\n",
        "\n",
        "for i, img in enumerate(integrated_grads):\n",
        "  if i == 5:\n",
        "    axs[3, i].set_title(\"Integrated Grad\")\n",
        "  axs[3, i].imshow(np.moveaxis(normalize(img),0,-1))\n",
        "  # axs[3, i].xaxis.set_visible(False)\n",
        "  axs[3, i].yaxis.set_visible(False)\n",
        "  axs[3, i].set_xlabel(f\"DAUC:{round(Integrated_Grad_dict_detailed['DAUC'][i], 2)}\\n IAUC:{round(Integrated_Grad_dict_detailed['IAUC'][i], 2)} \\n DC:{round(Integrated_Grad_dict_detailed['DC'][i], 2)}\")\n",
        "\n",
        "for i, img in enumerate(gradcam_expl):\n",
        "  if i == 5:\n",
        "    axs[4, i].set_title(\"Grad CAM\")\n",
        "  axs[4, i].imshow(img.detach().cpu().permute(1, 2, 0))\n",
        "  # axs[4, i].xaxis.set_visible(False)\n",
        "  axs[4, i].yaxis.set_visible(False)\n",
        "  axs[4, i].set_xlabel(f\"DAUC:{round(GradCAM_dict_detailed['DAUC'][i], 2)}\\n IAUC:{round(GradCAM_dict_detailed['IAUC'][i], 2)} \\n DC:{round(GradCAM_dict_detailed['DC'][i], 2)}\")"
      ],
      "metadata": {
        "id": "RiNlNf6Z222Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XAI Part 3 Grad_CAM Level Comparison\n"
      ],
      "metadata": {
        "id": "703weQqtLCeI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explore Grad-CAM's explanation performance across different hidden layers."
      ],
      "metadata": {
        "id": "7-2pLG1UVSjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_GradCAMscore(images, labels, model, level_id):\n",
        "\n",
        "  allImg = []\n",
        "  allExpl = []\n",
        "  allInds = []\n",
        "\n",
        "  for ind, img in enumerate(images):\n",
        "\n",
        "      img = img.unsqueeze(0)\n",
        "      class_ind = model(img.cuda()).argmax(dim=-1)\n",
        "\n",
        "      # Get the exaplanation map, should be of shape (1, 1, 128, 128) <-> (N, C, H, W)\n",
        "      explanation,_ = compute_gradcam(img, class_ind, level_id, model)\n",
        "      allImg.append(img.cuda())\n",
        "      allExpl.append(explanation)\n",
        "      allInds.append(class_ind)\n",
        "\n",
        "  allImg = torch.cat(allImg,dim=0)\n",
        "  allExpl = torch.cat(allExpl,dim=0)\n",
        "  allInds = torch.cat(allInds,dim=0)\n",
        "\n",
        "\n",
        "  deletion = Deletion()\n",
        "  result_dic = deletion(model,allImg.clone(),allExpl.clone(),allInds)\n",
        "  dauc_mean = result_dic[\"dauc\"]\n",
        "  dc_mean = result_dic[\"dc\"]\n",
        "\n",
        "  insertion = Insertion()\n",
        "  result_dic = insertion(model,allImg.clone(),allExpl.clone(),allInds)\n",
        "  iauc_mean = result_dic[\"iauc\"]\n",
        "  ic_mean = result_dic[\"ic\"]\n",
        "\n",
        "\n",
        "  iic_ad = IIC_AD()\n",
        "  result_dic = iic_ad(model,allImg,allExpl,allInds)\n",
        "  iic_mean,ad_mean = result_dic[\"iic\"],result_dic[\"ad\"]\n",
        "\n",
        "  add = ADD()\n",
        "  result_dic = add(model,allImg,allExpl,allInds)\n",
        "  add_mean = result_dic[\"add\"]\n",
        "\n",
        "\n",
        "  result_dict = {\n",
        "            \"DAUC\": dauc_mean\n",
        "            ,\"IAUC\": iauc_mean\n",
        "            ,\"DC\": dc_mean\n",
        "            ,\"IC\":ic_mean\n",
        "            ,\"IIC\": iic_mean\n",
        "            ,\"AD\": ad_mean\n",
        "            ,\"ADD\": add_mean\n",
        "          }\n",
        "\n",
        "\n",
        "  return result_dict"
      ],
      "metadata": {
        "id": "5V0UFsALLN6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.cnn"
      ],
      "metadata": {
        "id": "vk7okdV3UNtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "for id, layer in enumerate(model.cnn):\n",
        "  if isinstance(layer, torch.nn.Conv2d):\n",
        "    result_dic = get_GradCAMscore(images, labels, model, id)\n",
        "    results.append((id, result_dic))"
      ],
      "metadata": {
        "id": "i2_jmrgTLzs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for id, result_dic in results:\n",
        "  print(id, result_dic)"
      ],
      "metadata": {
        "id": "W17ixn7dMBKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.cnn\n"
      ],
      "metadata": {
        "id": "CfOXtAy6WBu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r0HkfWbxI5ST"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}